{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8df3eb-ee86-4fea-be63-08483a01d762",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ffe49-21fd-4d4a-81b6-84de3b81bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c88f8f-1315-4c46-9128-bd9239cc8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \"unstructured[md]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49026817-342d-4f26-914f-659be97cc556",
   "metadata": {},
   "source": [
    "### Setup database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5370a62-2278-48f0-89ef-b0c780cd1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import nltk\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a852b3b-c1b0-40bd-8479-fc515496374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2925cc35-36f1-4b8c-9dc9-1031c51fb045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,)\n",
    "chunks = text_splitter.split_documents(load_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b17a80a1-8a61-4685-ae32-e2c42f277d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = \"chroma\"\n",
    "load_dotenv()\n",
    "openai.api_key = os.environ['OPENAI_API_KEY'] # Will need .env file with API key\n",
    "db = Chroma.from_documents(chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce21640-45db-4f3c-ad17-e6c35292fb35",
   "metadata": {},
   "source": [
    "### Query database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ec476ee-908a-40f5-971c-205a63b74079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eaca444-14ba-4312-aca3-69bf20512ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d44ba44-6cdd-410d-8333-d3d832ea3e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Response(query):\n",
    "    results = db.similarity_search_with_relevance_scores(query, k=3)\n",
    "    if len(results) == 0 or results[0][1] < 0.7:\n",
    "        print(f\"Unable to find matching results.\")\n",
    "    \n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query)\n",
    "    print(prompt)\n",
    "    \n",
    "    model = ChatOpenAI()\n",
    "    response_text = model.invoke(prompt)\n",
    "    \n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8c7460b-c98f-47dd-a379-0335faa1095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "NLTK defines an infrastructure that can be used to build NLP programs in Python. It provides basic classes for representing data relevant to natural language processing; standard interfaces for performing tasks such as part-of-speech tagging, syntactic parsing, and text classification; and standard\n",
      "\n",
      "---\n",
      "\n",
      "The book is based on the Python programming language together with an open source library called the Natural Language Toolkit (NLTK). NLTK includes extensive software, data, and documentation, all freely downloadable from http://nltk.org/. Distributions are provided for Windows, Macintosh and Unix\n",
      "\n",
      "---\n",
      "\n",
      "NLTK was originally created in 2001 as part of a computational linguistics course in the Department of Computer and Information Science at the University of Pennsylvania. Since then it has been developed and expanded with the help of dozens of contributors. It has now been adopted in courses in\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What is NLTK?\n",
      "\n",
      "Response: content='NLTK is an open source library for the Python programming language that provides infrastructure for building NLP programs. It includes classes for representing data relevant to NLP, interfaces for tasks such as part-of-speech tagging and text classification, and extensive software, data, and documentation.' response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 204, 'total_tokens': 260, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-01a4b59f-1816-4ecb-bf3f-b14d6a0e1430-0' usage_metadata={'input_tokens': 204, 'output_tokens': 56, 'total_tokens': 260}\n",
      "Sources: ['data\\\\0. Preface.md', 'data\\\\0. Preface.md', 'data\\\\0. Preface.md']\n"
     ]
    }
   ],
   "source": [
    "Response(\"What is NLTK?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a4e9519-b1fe-4d5c-91af-0d3fd6611c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "Contrast this situation with frequency distributions (3), where we specify a word, and get back a number, e.g. fdist['monstrous'], which tells us the number of times a given word has occurred in a text. Look-up using words is familiar to anyone who has used a dictionary. Some more examples are\n",
      "\n",
      "---\n",
      "\n",
      "../images/tally.png\n",
      "Figure 3.1: Counting Words Appearing in a Text (a frequency distribution)\n",
      "\n",
      "---\n",
      "\n",
      "A frequency distribution is a collection of items along with their frequency counts (e.g., the words of a text and their frequency of appearance).\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: How would I get a word frequency distribution?\n",
      "\n",
      "Response: content=\"To get a word frequency distribution, you would specify a word and use the notation fdist['word'] to retrieve the number of times that word has occurred in a text. This concept is similar to looking up a word in a dictionary.\" response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 161, 'total_tokens': 210, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-61aa1292-bf6a-47ff-b29c-5d1b46c48062-0' usage_metadata={'input_tokens': 161, 'output_tokens': 49, 'total_tokens': 210}\n",
      "Sources: ['data\\\\5. Categorizing and Tagging Words.md', 'data\\\\1. Language Processing and Python.md', 'data\\\\1. Language Processing and Python.md']\n"
     ]
    }
   ],
   "source": [
    "Response(\"How would I get a word frequency distribution?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d342209-e5f6-4edb-bdce-d86298b5ebde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
